{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "batch1 = pd.read_csv(\"/Users/cjz/Downloads/HC AI intern (Trevor)/ACTUAL/Task3/input_csv/Batch1_Extraction_IssueCategorization - Batch1_Ground_Truth.csv\")\n",
    "batch2 = pd.read_csv(\"/Users/cjz/Downloads/HC AI intern (Trevor)/ACTUAL/Task3/input_csv/Batch2_Extraction_IssueCategorization - Batch2_GT.csv\")\n",
    "\n",
    "batch1 = batch1.rename(columns={batch1.columns[i]: batch1.columns[i].lower() for i in range(len(batch1.columns))})\n",
    "batch2 = batch2.rename(columns={batch2.columns[i]: batch2.columns[i].lower() for i in range(len(batch2.columns))})\n",
    "\n",
    "batch1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sender', 'model_extraction', 'dimension_uom', 'package_weight_uom', 'total_volume_uom', 'delivery_date', 'done by']\n",
      "['origin_state', 'origin_state_code', 'destination_location', 'destination_state', 'destination_state_code', 'container_details', 'cargo_details_dimension_uom', 'cargo_details_is_stackable', 'cargo_details_is_hazardous', 'cargo_details_package_type', 'cargo_details_package_weight_uom', 'volume_uom'] \n",
      "\n",
      "Index(['file', 'sender', 'date', 'email subject', 'email body',\n",
      "       'model_extraction', 'data', 'origin_city', 'origin_zip',\n",
      "       'origin_country', 'origin_country_code', 'destination_city',\n",
      "       'destination_zip', 'destination_country', 'destination_country_code',\n",
      "       'cargo_details_number_of_packages', 'cargo_details', 'dimension_uom',\n",
      "       'package_weight', 'package_weight_uom', 'total_package_count',\n",
      "       'gross_weight', 'gross_weight_uom', 'total_volume', 'total_volume_uom',\n",
      "       'incoterms', 'delivery_date', 'done by'],\n",
      "      dtype='object')\n",
      "Index(['file', 'date', 'email subject', 'email body', 'data', 'origin_city',\n",
      "       'origin_state', 'origin_state_code', 'origin_zip', 'origin_country',\n",
      "       'origin_country_code', 'destination_location', 'destination_city',\n",
      "       'destination_state', 'destination_state_code', 'destination_zip',\n",
      "       'destination_country', 'destination_country_code', 'container_details',\n",
      "       'cargo_details', 'cargo_details_dimension_uom', 'total_package_count',\n",
      "       'cargo_details_is_stackable', 'cargo_details_is_hazardous',\n",
      "       'cargo_details_package_type', 'cargo_details_number_of_packages',\n",
      "       'package_weight', 'cargo_details_package_weight_uom', 'gross_weight',\n",
      "       'gross_weight_uom', 'total_volume', 'volume_uom', 'incoterms'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df1_cols = list(batch1.columns)\n",
    "df2_cols = list(batch2.columns)\n",
    "\n",
    "# df1 = columns from batch 1; df2 = columns from batch 2\n",
    "verified_cols = []\n",
    "\n",
    "if len(df1_cols) < len(df2_cols): # check if there are any columns already by the same name\n",
    "    for col in df1_cols: \n",
    "        if col in df2_cols:\n",
    "            verified_cols.append(col)\n",
    "else:\n",
    "    for col in df2_cols:\n",
    "        if col in df1_cols:\n",
    "            verified_cols.append(col)\n",
    "\n",
    "for col in verified_cols: # remove these cols from each list\n",
    "    df1_cols.remove(col)\n",
    "    df2_cols.remove(col)\n",
    "\n",
    "print(df1_cols)\n",
    "print(df2_cols, \"\\n\")\n",
    "\n",
    "print(batch1.columns)\n",
    "print(batch2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# longest common substring algorithm - checkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCHES\n",
      "['sender']\n",
      "['model_extraction']\n",
      "['dimension_uom', ['cargo_details_dimension_uom', 13]]\n",
      "['package_weight_uom', ['cargo_details_package_type', 8], ['cargo_details_package_weight_uom', 18]]\n",
      "['total_volume_uom', ['volume_uom', 10]]\n",
      "['delivery_date']\n",
      "['done by']\n",
      "VERIFY\n",
      "['file', 'date', 'email subject', 'email body', 'data', 'origin_city', 'origin_zip', 'origin_country', 'origin_country_code', 'destination_city', 'destination_zip', 'destination_country', 'destination_country_code', 'cargo_details_number_of_packages', 'cargo_details', 'package_weight', 'total_package_count', 'gross_weight', 'gross_weight_uom', 'total_volume', 'incoterms', ['dimension_uom', 'cargo_details_dimension_uom'], ['package_weight_uom', 'cargo_details_package_weight_uom'], ['total_volume_uom', 'volume_uom']]\n",
      "DF1\n",
      "['sender', 'model_extraction', 'delivery_date', 'done by']\n",
      "DF2\n",
      "['origin_state', 'origin_state_code', 'destination_location', 'destination_state', 'destination_state_code', 'container_details', 'cargo_details_is_stackable', 'cargo_details_is_hazardous', 'cargo_details_package_type']\n"
     ]
    }
   ],
   "source": [
    "matches = [[match] for match in df1_cols]\n",
    "\n",
    "counter = -1\n",
    "\n",
    "# algorithm to find longest common substring\n",
    "for col1 in df1_cols:\n",
    "    counter += 1\n",
    "    for col2 in df2_cols:\n",
    "        matrix = [[0 for i in range(len(col1))] for j in range(len(col2))] # two dimensions to represent two words\n",
    "        highest = 0 # var to store length of longest substring\n",
    "        for loc1 in range(len(col1)):\n",
    "            for loc2 in range(len(col2)):\n",
    "                if col1[loc1] == col2[loc2]:\n",
    "                    matrix[loc2][loc1] = matrix[loc2 - 1][loc1 - 1] + 1\n",
    "                if matrix[loc2][loc1] > highest:\n",
    "                    highest = matrix[loc2][loc1]\n",
    "        if highest > 5:\n",
    "            matches[counter].append([col2, highest])\n",
    " \n",
    "print(\"MATCHES\")           \n",
    "for match in matches:\n",
    "    print(match)\n",
    "\n",
    "for match in matches:\n",
    "    if len(match) == 2:\n",
    "        verified_cols.append([match[0], match[1][0]])\n",
    "        df1_cols.remove(match[0])\n",
    "        df2_cols.remove(match[1][0])\n",
    "    elif len(match) > 2:\n",
    "        highest_list = [match[i][1] for i in range(1, len(match))]\n",
    "        max_idx = 0\n",
    "        for highest in range(len(highest_list)):\n",
    "            if highest_list[highest] > max_idx:\n",
    "                max_idx = highest\n",
    "        verified_cols.append([match[0], match[highest + 1][0]])\n",
    "        df1_cols.remove(match[0])\n",
    "        df2_cols.remove(match[highest + 1][0])\n",
    "        \n",
    "print(\"VERIFY\")\n",
    "print(verified_cols)\n",
    "print(\"DF1\")\n",
    "print(df1_cols)\n",
    "print(\"DF2\")\n",
    "print(df2_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# longest common substring algorithm - efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dimension_uom', 'cargo_details_dimension_uom', 13], ['package_weight_uom', 'cargo_details_package_type', 8], ['package_weight_uom', 'cargo_details_package_weight_uom', 18], ['total_volume_uom', 'volume_uom', 10]]\n",
      "[[[0, 13]], [[0, 13]], [[1, 8], [2, 18]], [[1, 8]], [[2, 18]], [[3, 10]], [[3, 10]]]\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m elem2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matches[elem])):\n\u001b[1;32m     53\u001b[0m                             \u001b[38;5;28;01mif\u001b[39;00m matches[elem][elem2] \u001b[38;5;241m==\u001b[39m matches[idx][idx2]:\n\u001b[0;32m---> 54\u001b[0m                                 \u001b[43mmatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(matches)\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "#matches = [[match] for match in df1_cols]\n",
    "\n",
    "counter = -1\n",
    "valid = []\n",
    "\n",
    "# algorithm to find longest common substring\n",
    "for col1 in df1_cols:\n",
    "    counter += 1\n",
    "    for col2 in df2_cols:\n",
    "        matrix = [[0 for i in range(len(col1))] for j in range(len(col2))] # two dimensions to represent two words\n",
    "        highest = 0 # var to store length of longest substring\n",
    "        for loc1 in range(len(col1)):\n",
    "            for loc2 in range(len(col2)):\n",
    "                if col1[loc1] == col2[loc2]:\n",
    "                    matrix[loc2][loc1] = matrix[loc2 - 1][loc1 - 1] + 1\n",
    "                if matrix[loc2][loc1] > highest:\n",
    "                    highest = matrix[loc2][loc1]\n",
    "        if highest > 6:\n",
    "            valid.append([col1, col2, highest])\n",
    "\n",
    "print(valid)\n",
    "\n",
    "unique = []\n",
    "\n",
    "\n",
    "for group in valid:\n",
    "    for loc in range(2):\n",
    "        if group[loc] not in unique:\n",
    "            unique.append(group[loc])\n",
    "            \n",
    "matches = []\n",
    "for idx in range(len(unique)):\n",
    "    #matches.append([])\n",
    "    for group in range(len(valid)):\n",
    "        if unique[idx] in valid[group]:\n",
    "            matches[idx] += [[group, valid[group][2]]]\n",
    "    if len(matches) == 2:\n",
    "        pass\n",
    "    \n",
    "print(matches)\n",
    "\n",
    "print(len(max(matches, key=len)))\n",
    "while len(max(matches, key=len)) > 1:\n",
    "    for idx in range(len(matches)):\n",
    "        if len(matches[idx]) == len(max(matches, key=len)):\n",
    "            max_idx = 0\n",
    "            for idx2 in range(len(matches[idx])-1):\n",
    "                if matches[idx][idx2][1] > matches[idx][max_idx][1]:\n",
    "                    max_idx = idx2\n",
    "                else:\n",
    "                    for elem in range(len(matches)):\n",
    "                        for elem2 in range(len(matches[elem])):\n",
    "                            if matches[elem][elem2] == matches[idx][idx2]:\n",
    "                                matches.remove(matches[elem][elem2])\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# individual example checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 3, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 4, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 5, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 6, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 7, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 8, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 9, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 10]\n"
     ]
    }
   ],
   "source": [
    "col1 = \"volume_uom\"\n",
    "col2 = \"total_volume_uom\"\n",
    "\n",
    "matrix = [[0 for i in range(len(col1))] for j in range(len(col2))]\n",
    "highest = 0\n",
    "for loc1 in range(len(col1)):\n",
    "    for loc2 in range(len(col2)):\n",
    "        if col1[loc1] == col2[loc2]:\n",
    "            matrix[loc2][loc1] = matrix[loc2 - 1][loc1 - 1] + 1\n",
    "        if matrix[loc2][loc1] > highest:\n",
    "            highest = matrix[loc2][loc1]\n",
    "print(highest)\n",
    "            \n",
    "\n",
    "for row in matrix:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# editing existing dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sender', 'model_extraction', 'dimension_uom', 'package_weight_uom', 'total_volume_uom', 'delivery_date', 'done by']\n",
      "['origin_state', 'origin_state_code', 'destination_location', 'destination_state', 'destination_state_code', 'container_details', 'cargo_details_dimension_uom', 'cargo_details_is_stackable', 'cargo_details_is_hazardous', 'cargo_details_package_type', 'cargo_details_package_weight_uom', 'volume_uom']\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(verified_cols) - 1, 0, -1):\n",
    "    if type(verified_cols[idx]) == list and len(verified_cols[idx]) == 2:\n",
    "        batch2.rename(columns={verified_cols[idx][1]: verified_cols[idx][0]})\n",
    "    else:\n",
    "        break\n",
    "\n",
    "try:\n",
    "    for col in df1_cols:\n",
    "        del batch1[col]\n",
    "        \n",
    "    for col in df2_cols:\n",
    "        del batch2[col]\n",
    "except: pass\n",
    "    \n",
    "\"\"\"print(len(batch1.columns))\n",
    "print(batch1.columns)\n",
    "print(len(batch2.columns))\n",
    "print(batch2.columns, \"\\n\")\n",
    "print(df1_cols)\n",
    "print(df2_cols)\"\"\"\n",
    "\n",
    "for col in batch1.columns:\n",
    "    if col not in batch2.columns:\n",
    "        df1_cols.append(col)\n",
    "        del batch1[col]\n",
    "        \n",
    "for col in batch2.columns:\n",
    "    if col not in batch1.columns:\n",
    "        df2_cols.append(col)\n",
    "        del batch2[col]\n",
    "        \n",
    "print(df1_cols)\n",
    "print(df2_cols)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
